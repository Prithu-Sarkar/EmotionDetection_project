{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Convolutional Neural Network with Batch-Normalization </h1> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Objective<h3>    \n",
    "<h5>Learn how to compare a Convolutional Neural Network using Batch Normalization with a regular Convolutional Neural Network  to classify handwritten digits from the MNIST database.</h5>     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<b>This lab takes a long time to run so the results are given. You can run the notebook your self but it may take a long time.</b>\n",
    "<p>In this lab, we will compare a Convolutional Neural Network using Batch Normalization with a regular Convolutional Neural Network  to classify handwritten digits from the MNIST database. We will reshape the images to make them faster to process. </p>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#read_me\">Read me Batch Norm for Convolution Operation  </a></li>\n",
    "<li><a href=\"#Makeup_Data\">Get Some Data</a></li>\n",
    "<li><a href=\"#CNN\">Two Types of Convolutional Neural Network</a></li>\n",
    "<li><a href=\"#Train\">Define Criterion function, Optimizer and Train the Model</a></li>\n",
    "<li><a href=\"#Result\">Analyze Results</a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>25 min</strong></p>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"read_me\"><h2 id=\"read_me\">Read me Batch Norm for Convolution Operation  </h2></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like a fully connected network, we create a <code>BatchNorm2d</code> object, but we apply it to the 2D convolution object. First, we create objects <code>Conv2d</code> object; we require the number of output channels, specified by the variable <code>OUT</code>.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>self.cnn1 = nn.Conv2d(in_channels=1, out_channels=OUT, kernel_size=5, padding=2) </code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a Batch Norm  object for 2D convolution as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>self.conv1_bn = nn.BatchNorm2d(OUT)</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter out is the number of channels in the output. We can then apply batch norm  after  the convolution operation :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>x = self.cnn1(x)</code>\n",
    " <code> x=self.conv1_bn(x)</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preparation</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-12.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.3.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m139.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-12.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.3.2-py3-none-any.whl (122 kB)\n",
      "Installing collected packages: pyparsing, pillow, numpy, kiwisolver, fonttools, cycler, pandas, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 numpy-2.4.2 pandas-3.0.0 pillow-12.1.1 pyparsing-3.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch==2.8.0+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.8.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchvision==0.23.0+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.23.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio==2.8.0+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.8.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
      "Collecting filelock (from torch==2.8.0+cpu)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0+cpu) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0+cpu) (75.8.0)\n",
      "Collecting sympy>=1.13.3 (from torch==2.8.0+cpu)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.8.0+cpu)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0+cpu) (3.1.5)\n",
      "Collecting fsspec (from torch==2.8.0+cpu)\n",
      "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision==0.23.0+cpu) (2.4.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision==0.23.0+cpu) (12.1.1)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch==2.8.0+cpu)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.8.0+cpu) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torch-2.8.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl (183.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading https://download.pytorch.org/whl/cpu/torchvision-0.23.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.8.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m124.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch, torchvision, torchaudio\n",
      "Successfully installed filelock-3.20.0 fsspec-2025.12.0 mpmath-1.3.0 networkx-3.6.1 sympy-1.14.0 torch-2.8.0+cpu torchaudio-2.8.0+cpu torchvision-0.23.0+cpu\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 857 ms, sys: 181 ms, total: 1.04 s\n",
      "Wall time: 50 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install pandas numpy matplotlib\n",
    "%pip install torch==2.8.0+cpu torchvision==0.23.0+cpu torchaudio==2.8.0+cpu \\\n",
    "    --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import the libraries we need to use in this lab\n",
    "\n",
    "# Using the following line code to install the torchvision library\n",
    "# !mamba install -y torchvision\n",
    "\n",
    "#!pip install torchvision==0.9.1 torch==1.8.1 \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "def show_data(data_sample):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n",
    "    plt.title('y = '+ str(data_sample[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Makeup_Data\"><h2 id=\"Makeup_Data\">Get the Data</h2> </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a transform to resize the image and convert it to a tensor :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_SIZE = 16\n",
    "\n",
    "composed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training dataset by setting the parameters <code>train </code> to <code>True</code>. We use the transform defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing dataset by setting the parameters train  <code>False</code>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the validating \n",
    "\n",
    "validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the data type is long.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the data type for each element in dataset\n",
    "\n",
    "type(train_dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in the rectangular tensor corresponds to a number representing a pixel intensity as demonstrated by the following image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.2.1imagenet.png\" width=\"550\" alt=\"MNIST data image\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the fourth label \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The label for the fourth data element\n",
    "\n",
    "train_dataset[3][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fourth sample \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The image for the fourth data element\n",
    "show_data(train_dataset[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth sample is a \"1\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"CNN\"><h2 id=\"CNN\">Build a Two Convolutional Neural Network Class</h2></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    # Contructor\n",
    "    def __init__(self, out_1=16, out_2=32):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)\n",
    "    \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Convolutional Network class with two Convolutional layers and one fully connected layer. But we add Batch Norm for the convolutional layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_batch(nn.Module):\n",
    "    \n",
    "    # Contructor\n",
    "    def __init__(self, out_1=16, out_2=32,number_of_classes=10):\n",
    "        super(CNN_batch, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n",
    "        self.conv1_bn = nn.BatchNorm2d(out_1)\n",
    "\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2_bn = nn.BatchNorm2d(out_2)\n",
    "\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(out_2 * 4 * 4, number_of_classes)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(10)\n",
    "    \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x=self.conv1_bn(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x=self.conv2_bn(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x=self.bn_fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_loader,validation_loader,optimizer,n_epochs=4):\n",
    "    \n",
    "    #global variable \n",
    "    N_test=len(validation_dataset)\n",
    "    accuracy_list=[]\n",
    "    loss_list=[]\n",
    "    for epoch in range(n_epochs):\n",
    "        for x, y in train_loader:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            z = model(x)\n",
    "            loss = criterion(z, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_list.append(loss.data)\n",
    "\n",
    "        correct=0\n",
    "        #perform a prediction on the validation  data  \n",
    "        for x_test, y_test in validation_loader:\n",
    "            model.eval()\n",
    "            z = model(x_test)\n",
    "            _, yhat = torch.max(z.data, 1)\n",
    "            correct += (yhat == y_test).sum().item()\n",
    "        accuracy = correct / N_test\n",
    "        accuracy_list.append(accuracy)\n",
    "     \n",
    "    return accuracy_list, loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Train\"><h2 id=\"Train\">Define the Convolutional Neural Network Classifier, Criterion function, Optimizer and Train the Model</h2> </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 16 output channels for the first layer, and 32 output channels for the second layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model object using CNN class\n",
    "model = CNN(out_1=16, out_2=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function, the optimizer and the dataset loader \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "accuracy_list_normal, loss_list_normal=train_model(model=model,n_epochs=10,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the Process for the model with  batch norm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batch=CNN_batch(out_1=16, out_2=32)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model_batch.parameters(), lr = learning_rate)\n",
    "accuracy_list_batch, loss_list_batch=train_model(model=model_batch,n_epochs=10,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Result\"><h2 id=\"Result\">Analyze Results</h2> </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss with both networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy\n",
    "\n",
    "plt.plot(loss_list_normal, 'b',label='loss normal cnn ')\n",
    "plt.plot(loss_list_batch,'r',label='loss batch cnn')\n",
    "plt.xlabel('iteration')\n",
    "plt.title(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracy_list_normal, 'b',label=' normal CNN')\n",
    "plt.plot(accuracy_list_batch,'r',label=' CNN with Batch Norm')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title(\"Accuracy \")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the CNN with batch norm performers better, with faster convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other contributors: <a href=\"https://www.linkedin.com/in/michelleccarey/\">Michelle Carey</a>, <a href=\"https://www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\">Mavis Zhou</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to Magnus <a href=\"http://www.hvass-labs.org/\">Erik Hvass Pedersen</a> whose tutorials helped me understand convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "\n",
    "\n",
    "\n",
    "<hr>-->\n",
    "\n",
    "## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "b9f65cfa3e585c8a3b6b4481574a1ed00dc7400f94a2d344b785ffa602e7c072"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
