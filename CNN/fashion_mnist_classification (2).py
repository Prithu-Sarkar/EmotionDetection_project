# -*- coding: utf-8 -*-
"""Fashion MNIST Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/184laKwACk9ZX9Be4vCAA_G4WWcBtYbcK
"""

# ============================================================
# FASHION-MNIST CNN CLASSIFICATION - GOOGLE COLAB (GPU READY)
# ============================================================

# â”€â”€ STEP 0: Verify GPU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import torch
print("PyTorch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# â”€â”€ STEP 1: Import Libraries â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import time

print("\nâœ… All libraries imported successfully!")

# â”€â”€ STEP 2: Define Class Labels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class_names = [
    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'
]
print(f"\nğŸ“¦ Fashion-MNIST Classes ({len(class_names)} total):")
for i, name in enumerate(class_names):
    print(f"  {i}: {name}")

# ============================================================
# â”€â”€ STEP 3: MNIST VALIDATION IMAGE â†’ RESIZE TO 16Ã—16 â”€â”€â”€â”€â”€â”€â”€
# (Resize MNIST validation image to 16x16, convert to tensor,
#  display the first sample) [REQUIRED CRITERION - p1b1]
# ============================================================
print("\n" + "="*60)
print("STEP 3: MNIST â€” Resize to 16Ã—16 & Display First Sample")
print("="*60)

mnist_transform_16 = transforms.Compose([
    transforms.Resize((16, 16)),
    transforms.ToTensor()
])

mnist_val_dataset = torchvision.datasets.MNIST(
    root='./data', train=False, download=True, transform=mnist_transform_16
)

# Get the FIRST sample
mnist_first_image, mnist_first_label = mnist_val_dataset[0]

print(f"\nâœ… MNIST Validation Dataset loaded: {len(mnist_val_dataset)} samples")
print(f"   First sample tensor shape : {mnist_first_image.shape}  (CÃ—HÃ—W)")
print(f"   Pixel value range         : [{mnist_first_image.min():.3f}, {mnist_first_image.max():.3f}]")
print(f"   First sample label        : {mnist_first_label} (digit '{mnist_first_label}')")

# Display the first MNIST sample resized to 16Ã—16
fig, axes = plt.subplots(1, 2, figsize=(8, 4))
fig.suptitle("MNIST Validation â€” First Sample Resized to 16Ã—16", fontsize=13, fontweight='bold')

# Left: resized 16Ã—16
axes[0].imshow(mnist_first_image.squeeze().numpy(), cmap='gray')
axes[0].set_title(f"16Ã—16 Resized\nLabel: {mnist_first_label}", fontsize=11)
axes[0].axis('off')

# Right: original 28Ã—28 for comparison
mnist_orig = torchvision.datasets.MNIST(
    root='./data', train=False, download=False,
    transform=transforms.ToTensor()
)
orig_img, _ = mnist_orig[0]
axes[1].imshow(orig_img.squeeze().numpy(), cmap='gray')
axes[1].set_title(f"Original 28Ã—28\nLabel: {mnist_first_label}", fontsize=11)
axes[1].axis('off')

plt.tight_layout()
plt.show()

print(f"\nğŸ“‹ Tensor Info for First MNIST Sample:")
print(f"   Shape  : {mnist_first_image.shape}")
print(f"   Dtype  : {mnist_first_image.dtype}")
print(f"   Label  : {mnist_first_label}")

# ============================================================
# â”€â”€ STEP 4: FashionMNIST â†’ RESIZE TO 16Ã—16, DISPLAY 3 SAMPLES
# (Resize FashionMNIST images to 16x16, convert to tensors,
#  display first THREE validation samples) [REQUIRED - p1b1]
# ============================================================
print("\n" + "="*60)
print("STEP 4: FashionMNIST â€” Resize to 16Ã—16 & Display First 3 Validation Samples")
print("="*60)

fashion_transform_16 = transforms.Compose([
    transforms.Resize((16, 16)),
    transforms.ToTensor()
])

fashion_val_16 = torchvision.datasets.FashionMNIST(
    root='./data', train=False, download=True, transform=fashion_transform_16
)

print(f"\nâœ… FashionMNIST Validation Dataset loaded: {len(fashion_val_16)} samples")

# Display first THREE validation samples at 16Ã—16
fig, axes = plt.subplots(1, 3, figsize=(10, 4))
fig.suptitle("FashionMNIST Validation â€” First 3 Samples Resized to 16Ã—16", fontsize=13, fontweight='bold')

for i in range(3):
    img, label = fashion_val_16[i]
    print(f"\n   Sample {i+1}:")
    print(f"     Tensor shape : {img.shape}  (CÃ—HÃ—W)")
    print(f"     Dtype        : {img.dtype}")
    print(f"     Label index  : {label} â†’ '{class_names[label]}'")
    print(f"     Value range  : [{img.min():.3f}, {img.max():.3f}]")

    axes[i].imshow(img.squeeze().numpy(), cmap='gray')
    axes[i].set_title(f"Sample {i+1}\nLabel: {label}\n{class_names[label]}", fontsize=10)
    axes[i].axis('off')

plt.tight_layout()
plt.show()

# â”€â”€ STEP 5: Full-Resolution Data Transforms for Training â”€â”€â”€â”€
print("\n" + "="*60)
print("STEP 5: Full-Resolution Transforms for CNN Training")
print("="*60)

train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(degrees=10),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

print("âœ… Train: RandomHorizontalFlip + RandomRotation + Normalize")
print("âœ… Test : Normalize only")

# â”€â”€ STEP 6: Load Full FashionMNIST for Training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "="*60)
print("STEP 6: Load FashionMNIST at 28Ã—28 for CNN Training")
print("="*60)

BATCH_SIZE = 64

train_dataset = torchvision.datasets.FashionMNIST(
    root='./data', train=True, download=True, transform=train_transform
)
test_dataset = torchvision.datasets.FashionMNIST(
    root='./data', train=False, download=True, transform=test_transform
)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,
                          num_workers=2, pin_memory=True)
test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,
                          num_workers=2, pin_memory=True)

print(f"âœ… Training samples : {len(train_dataset):,}")
print(f"   Test samples     : {len(test_dataset):,}")
print(f"   Batch size       : {BATCH_SIZE}")

# â”€â”€ STEP 7: Visualize Sample Images (28Ã—28) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
raw_test = torchvision.datasets.FashionMNIST(
    root='./data', train=False, download=False, transform=transforms.ToTensor()
)

def show_samples(dataset, class_names, n=10):
    fig, axes = plt.subplots(2, 5, figsize=(12, 5))
    fig.suptitle("FashionMNIST â€” Random Sample Images (28Ã—28)", fontsize=13, fontweight='bold')
    indices = np.random.choice(len(dataset), n, replace=False)
    for i, ax in enumerate(axes.flat):
        img, label = dataset[indices[i]]
        ax.imshow(img.squeeze().numpy(), cmap='gray')
        ax.set_title(class_names[label], fontsize=9)
        ax.axis('off')
    plt.tight_layout()
    plt.show()

show_samples(raw_test, class_names)

# â”€â”€ STEP 8: Define CNN Architecture â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "="*60)
print("STEP 8: CNN Architecture")
print("="*60)

class FashionCNN(nn.Module):
    def __init__(self):
        super(FashionCNN, self).__init__()

        self.conv_block1 = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.25)
        )

        self.conv_block2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.25)
        )

        self.conv_block3 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.25)
        )

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 3 * 3, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(128, 10)
        )

    def forward(self, x):
        x = self.conv_block1(x)
        x = self.conv_block2(x)
        x = self.conv_block3(x)
        x = self.classifier(x)
        return x

model = FashionCNN().to(device)
print(model)
total_params   = sum(p.numel() for p in model.parameters())
trainable      = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\nğŸ“Š Total parameters    : {total_params:,}")
print(f"   Trainable parameters: {trainable:,}")

# ============================================================
# â”€â”€ STEP 9: LOSS FUNCTION & OPTIMIZER (SGD lr=0.1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# criterion = nn.CrossEntropyLoss()                [REQUIRED]
# optimizer = SGD with lr=0.1                      [REQUIRED]
# ============================================================
print("\n" + "="*60)
print("STEP 9: Loss Function & Optimizer (SGD, lr=0.1)")
print("="*60)

NUM_EPOCHS = 10   # â‰¤ 10 epochs as required

# âœ… REQUIRED: Loss function named 'criterion'
criterion = nn.CrossEntropyLoss()

# âœ… REQUIRED: SGD optimizer named 'optimizer' with lr=0.1
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)

scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='max', factor=0.5, patience=2
)

print(f"âœ… criterion = nn.CrossEntropyLoss()")
print(f"âœ… optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)")
print(f"âœ… scheduler = ReduceLROnPlateau(factor=0.5, patience=2)")
print(f"âœ… Epochs    = {NUM_EPOCHS}")

# â”€â”€ STEP 10: Train & Eval Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def train_one_epoch(model, loader, criterion, optimizer, device):
    model.train()
    running_loss, correct, total = 0.0, 0, 0
    for images, labels in loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * images.size(0)
        _, predicted = outputs.max(1)
        correct += predicted.eq(labels).sum().item()
        total   += labels.size(0)
    return running_loss / total, 100. * correct / total


def evaluate(model, loader, criterion, device):
    model.eval()
    running_loss, correct, total = 0.0, 0, 0
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            running_loss += loss.item() * images.size(0)
            _, predicted = outputs.max(1)
            correct += predicted.eq(labels).sum().item()
            total   += labels.size(0)
    return running_loss / total, 100. * correct / total

# â”€â”€ STEP 11: Training Loop (10 Epochs) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "="*60)
print("STEP 11: Training Loop â€” 10 Epochs")
print("="*60)

history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
best_val_acc    = 0.0
best_model_state = None

print(f"\nğŸš€ Training on {device}...\n")
print(f"{'Epoch':>5} {'Train Loss':>11} {'Train Acc':>10} {'Val Loss':>10} {'Val Acc':>9} {'Time':>7}")
print("â€”" * 60)

for epoch in range(1, NUM_EPOCHS + 1):
    t0 = time.time()
    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)
    val_loss,   val_acc   = evaluate(model, test_loader, criterion, device)
    scheduler.step(val_acc)

    history['train_loss'].append(train_loss)
    history['train_acc'].append(train_acc)
    history['val_loss'].append(val_loss)
    history['val_acc'].append(val_acc)

    elapsed = time.time() - t0
    print(f"{epoch:>5} {train_loss:>11.4f} {train_acc:>9.2f}% {val_loss:>10.4f} {val_acc:>8.2f}% {elapsed:>6.1f}s")

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        best_model_state = {k: v.clone() for k, v in model.state_dict().items()}
        print(f"        â­ Best model saved! Val Acc: {best_val_acc:.2f}%")

print(f"\nâœ… Training complete! Best Validation Accuracy: {best_val_acc:.2f}%")

# â”€â”€ STEP 12: Training Curves â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
fig.suptitle("Training History (SGD, lr=0.1, 10 Epochs)", fontsize=13, fontweight='bold')
epochs_range = range(1, NUM_EPOCHS + 1)

ax1.plot(epochs_range, history['train_loss'], 'b-o', label='Train Loss', markersize=5)
ax1.plot(epochs_range, history['val_loss'],   'r-o', label='Val Loss',   markersize=5)
ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss')
ax1.set_title('Loss Curves'); ax1.legend(); ax1.grid(True, alpha=0.3)

ax2.plot(epochs_range, history['train_acc'], 'b-o', label='Train Acc', markersize=5)
ax2.plot(epochs_range, history['val_acc'],   'r-o', label='Val Acc',   markersize=5)
ax2.set_xlabel('Epoch'); ax2.set_ylabel('Accuracy (%)')
ax2.set_title('Accuracy Curves'); ax2.legend(); ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# â”€â”€ STEP 13: Final Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model.load_state_dict(best_model_state)
final_loss, final_acc = evaluate(model, test_loader, criterion, device)
print(f"\nğŸ“Š Final Test Accuracy : {final_acc:.2f}%")
print(f"   Final Test Loss     : {final_loss:.4f}")

# â”€â”€ STEP 14: Classification Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ“‹ Classification Report:")
all_preds, all_labels = [], []
model.eval()
with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        outputs = model(images)
        _, preds = outputs.max(1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.numpy())

print(classification_report(all_labels, all_preds, target_names=class_names))

# â”€â”€ STEP 15: Confusion Matrix â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix', fontsize=14, fontweight='bold')
plt.ylabel('True Label'); plt.xlabel('Predicted Label')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# â”€â”€ STEP 16: Prediction Visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def show_predictions(model, dataset, class_names, n=20, device='cpu'):
    model.eval()
    indices = np.random.choice(len(dataset), n, replace=False)
    fig, axes = plt.subplots(4, 5, figsize=(14, 11))
    fig.suptitle("Model Predictions (Green=Correct, Red=Wrong)", fontsize=13, fontweight='bold')
    with torch.no_grad():
        for i, ax in enumerate(axes.flat):
            img, true_label = dataset[indices[i]]
            output = model(img.unsqueeze(0).to(device))
            pred_label = output.argmax(1).item()
            conf = torch.softmax(output, dim=1).max().item() * 100
            ax.imshow(img.squeeze().numpy(), cmap='gray')
            color = 'green' if pred_label == true_label else 'red'
            ax.set_title(
                f"Pred: {class_names[pred_label]}\nTrue: {class_names[true_label]}\n{conf:.1f}%",
                color=color, fontsize=7.5
            )
            ax.axis('off')
    plt.tight_layout()
    plt.show()

show_predictions(model, raw_test, class_names, n=20, device=device)

# â”€â”€ STEP 17: Per-Class Accuracy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)
per_class_acc = cm_norm.diagonal() * 100
print("\nğŸ“Š Per-Class Accuracy:")
print(f"{'Class':<15} {'Accuracy':>10}")
print("â€”" * 27)
for name, acc in zip(class_names, per_class_acc):
    bar = 'â–ˆ' * int(acc / 5)
    print(f"{name:<15} {acc:>8.1f}%  {bar}")

plt.figure(figsize=(10, 5))
colors = ['#2ecc71' if a >= 90 else '#e67e22' if a >= 80 else '#e74c3c' for a in per_class_acc]
bars = plt.bar(class_names, per_class_acc, color=colors)
plt.xticks(rotation=45, ha='right')
plt.ylabel('Accuracy (%)'); plt.ylim([60, 100])
plt.title('Per-Class Accuracy', fontsize=13, fontweight='bold')
plt.axhline(y=np.mean(per_class_acc), color='blue', linestyle='--',
            label=f'Mean: {np.mean(per_class_acc):.1f}%')
for bar, acc in zip(bars, per_class_acc):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,
             f'{acc:.1f}%', ha='center', va='bottom', fontsize=8)
plt.legend(); plt.tight_layout(); plt.show()

# â”€â”€ STEP 18: Save Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
torch.save({
    'model_state_dict'    : model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'best_val_acc'        : best_val_acc,
    'class_names'         : class_names,
    'architecture'        : 'FashionCNN'
}, 'fashion_mnist_cnn_best.pth')

print("\nâœ… Model saved as 'fashion_mnist_cnn_best.pth'")
print(f"\n{'='*55}")
print(f"  ğŸ‰ FINAL RESULTS SUMMARY")
print(f"{'='*55}")
print(f"  criterion              : nn.CrossEntropyLoss()   âœ…")
print(f"  optimizer              : SGD (lr=0.1)            âœ…")
print(f"  MNIST 16Ã—16 sample     : Displayed               âœ…")
print(f"  FashionMNIST 16Ã—16 Ã—3  : Displayed               âœ…")
print(f"  Epochs                 : {NUM_EPOCHS}                         âœ…")
print(f"  Best Val Accuracy      : {best_val_acc:.2f}%")
print(f"  Final Test Accuracy    : {final_acc:.2f}%")
print(f"  Device                 : {device}")
print(f"{'='*55}")