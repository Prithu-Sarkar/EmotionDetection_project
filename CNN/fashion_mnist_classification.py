# -*- coding: utf-8 -*-
"""Fashion MNIST Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/184laKwACk9ZX9Be4vCAA_G4WWcBtYbcK
"""

# ============================================================
# FASHION-MNIST CNN CLASSIFICATION - GOOGLE COLAB (GPU READY)
# ============================================================

# â”€â”€ STEP 0: Verify GPU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import torch
print("PyTorch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# â”€â”€ STEP 1: Import Libraries â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import time

print("\nâœ… All libraries imported successfully!")

# â”€â”€ STEP 2: Define Class Labels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class_names = [
    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'
]
print(f"\nğŸ“¦ Classes ({len(class_names)} total):")
for i, name in enumerate(class_names):
    print(f"  {i}: {name}")

# â”€â”€ STEP 3: Data Transforms â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Training: augmentation for better generalization
train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(degrees=10),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))   # mean=0.5, std=0.5
])

# Testing: only normalize (no augmentation)
test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

print("\nâœ… Transforms defined!")
print("  Train: RandomHorizontalFlip + RandomRotation + Normalize")
print("  Test : Normalize only")

# â”€â”€ STEP 4: Load Fashion-MNIST Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nâ¬‡ï¸  Downloading Fashion-MNIST dataset...")

train_dataset = torchvision.datasets.FashionMNIST(
    root='./data', train=True, download=True, transform=train_transform
)
test_dataset = torchvision.datasets.FashionMNIST(
    root='./data', train=False, download=True, transform=test_transform
)

# DataLoaders
BATCH_SIZE = 64

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,
                          num_workers=2, pin_memory=True)
test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,
                          num_workers=2, pin_memory=True)

print(f"\nâœ… Dataset loaded!")
print(f"  Training samples : {len(train_dataset):,}")
print(f"  Test samples     : {len(test_dataset):,}")
print(f"  Batch size       : {BATCH_SIZE}")
print(f"  Training batches : {len(train_loader)}")
print(f"  Test batches     : {len(test_loader)}")

"""# **Fashion MNIST Classification**"""

# â”€â”€ STEP 5: Visualize Sample Images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def show_samples(dataset, class_names, n=10):
    fig, axes = plt.subplots(2, 5, figsize=(12, 5))
    fig.suptitle("Fashion-MNIST Sample Images", fontsize=14, fontweight='bold')
    indices = np.random.choice(len(dataset), n, replace=False)
    for i, ax in enumerate(axes.flat):
        img, label = dataset[indices[i]]
        img = img.squeeze().numpy() * 0.5 + 0.5  # denormalize
        ax.imshow(img, cmap='gray')
        ax.set_title(class_names[label], fontsize=9)
        ax.axis('off')
    plt.tight_layout()
    plt.show()

# Use raw test dataset (no augmentation) for visualization
raw_test = torchvision.datasets.FashionMNIST(
    root='./data', train=False, download=False,
    transform=transforms.ToTensor()
)
show_samples(raw_test, class_names)

# â”€â”€ STEP 6: Define the CNN Architecture â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class FashionCNN(nn.Module):
    """
    CNN Architecture:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Input: 1Ã—28Ã—28                  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Conv Block 1: 32 filters, 3Ã—3   â”‚
    â”‚ BatchNorm â†’ ReLU â†’ MaxPool(2Ã—2) â”‚  â†’ 32Ã—14Ã—14
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Conv Block 2: 64 filters, 3Ã—3   â”‚
    â”‚ BatchNorm â†’ ReLU â†’ MaxPool(2Ã—2) â”‚  â†’ 64Ã—7Ã—7
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Conv Block 3: 128 filters, 3Ã—3  â”‚
    â”‚ BatchNorm â†’ ReLU â†’ MaxPool(2Ã—2) â”‚  â†’ 128Ã—3Ã—3
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Flatten â†’ FC(1152â†’256) â†’ Drop   â”‚
    â”‚ FC(256â†’128) â†’ Drop              â”‚
    â”‚ FC(128â†’10)  â† output            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    def __init__(self):
        super(FashionCNN, self).__init__()

        # â”€â”€ Convolutional Block 1 â”€â”€
        self.conv_block1 = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),   # 1Ã—28Ã—28 â†’ 32Ã—28Ã—28
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, kernel_size=3, padding=1),  # 32Ã—28Ã—28 â†’ 32Ã—28Ã—28
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),                            # â†’ 32Ã—14Ã—14
            nn.Dropout2d(0.25)
        )

        # â”€â”€ Convolutional Block 2 â”€â”€
        self.conv_block2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # â†’ 64Ã—14Ã—14
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # â†’ 64Ã—14Ã—14
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),                            # â†’ 64Ã—7Ã—7
            nn.Dropout2d(0.25)
        )

        # â”€â”€ Convolutional Block 3 â”€â”€
        self.conv_block3 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, padding=1), # â†’ 128Ã—7Ã—7
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),                            # â†’ 128Ã—3Ã—3
            nn.Dropout2d(0.25)
        )

        # â”€â”€ Fully Connected Head â”€â”€
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 3 * 3, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(128, 10)                            # 10 classes
        )

    def forward(self, x):
        x = self.conv_block1(x)
        x = self.conv_block2(x)
        x = self.conv_block3(x)
        x = self.classifier(x)
        return x

# Instantiate and move to device
model = FashionCNN().to(device)
print("\nâœ… Model Architecture:")
print(model)

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
trainable    = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\nğŸ“Š Total parameters    : {total_params:,}")
print(f"   Trainable parameters: {trainable:,}")

# â”€â”€ STEP 7: Loss, Optimizer & Scheduler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LEARNING_RATE = 0.001
NUM_EPOCHS    = 20

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='max', factor=0.5, patience=3
)

print(f"\nâœ… Training setup:")
print(f"  Loss function : CrossEntropyLoss")
print(f"  Optimizer     : Adam (lr={LEARNING_RATE}, weight_decay=1e-4)")
print(f"  Scheduler     : ReduceLROnPlateau (factor=0.5, patience=3)")
print(f"  Epochs        : {NUM_EPOCHS}")

# â”€â”€ STEP 8: Training & Validation Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def train_one_epoch(model, loader, criterion, optimizer, device):
    model.train()
    running_loss, correct, total = 0.0, 0, 0
    for images, labels in loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * images.size(0)
        _, predicted = outputs.max(1)
        correct += predicted.eq(labels).sum().item()
        total   += labels.size(0)
    return running_loss / total, 100. * correct / total


def evaluate(model, loader, criterion, device):
    model.eval()
    running_loss, correct, total = 0.0, 0, 0
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            running_loss += loss.item() * images.size(0)
            _, predicted = outputs.max(1)
            correct += predicted.eq(labels).sum().item()
            total   += labels.size(0)
    return running_loss / total, 100. * correct / total

print("\nâœ… Training and evaluation functions defined!")

# â”€â”€ STEP 9: Training Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
best_val_acc = 0.0
best_model_state = None

print(f"\nğŸš€ Starting training on {device}...\n")
print(f"{'Epoch':>5} {'Train Loss':>11} {'Train Acc':>10} {'Val Loss':>10} {'Val Acc':>9} {'Time':>7}")
print("â”€" * 60)

for epoch in range(1, NUM_EPOCHS + 1):
    t0 = time.time()

    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)
    val_loss,   val_acc   = evaluate(model, test_loader, criterion, device)

    scheduler.step(val_acc)

    history['train_loss'].append(train_loss)
    history['train_acc'].append(train_acc)
    history['val_loss'].append(val_loss)
    history['val_acc'].append(val_acc)

    elapsed = time.time() - t0
    print(f"{epoch:>5} {train_loss:>11.4f} {train_acc:>9.2f}% {val_loss:>10.4f} {val_acc:>8.2f}% {elapsed:>6.1f}s")

    # Save best model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        best_model_state = {k: v.clone() for k, v in model.state_dict().items()}
        print(f"        â­ New best model saved! Val Acc: {best_val_acc:.2f}%")

print(f"\nâœ… Training complete! Best Validation Accuracy: {best_val_acc:.2f}%")

# â”€â”€ STEP 10: Plot Training Curves â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
fig.suptitle("Training History", fontsize=14, fontweight='bold')
epochs = range(1, NUM_EPOCHS + 1)

ax1.plot(epochs, history['train_loss'], 'b-o', label='Train Loss', markersize=4)
ax1.plot(epochs, history['val_loss'],   'r-o', label='Val Loss',   markersize=4)
ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss')
ax1.set_title('Loss Curves'); ax1.legend(); ax1.grid(True, alpha=0.3)

ax2.plot(epochs, history['train_acc'], 'b-o', label='Train Acc', markersize=4)
ax2.plot(epochs, history['val_acc'],   'r-o', label='Val Acc',   markersize=4)
ax2.set_xlabel('Epoch'); ax2.set_ylabel('Accuracy (%)')
ax2.set_title('Accuracy Curves'); ax2.legend(); ax2.grid(True, alpha=0.3)
ax2.set_ylim([80, 100])

plt.tight_layout()
plt.show()

# â”€â”€ STEP 11: Load Best Model & Final Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model.load_state_dict(best_model_state)
final_loss, final_acc = evaluate(model, test_loader, criterion, device)
print(f"\nğŸ“Š Final Test Accuracy (best model): {final_acc:.2f}%")
print(f"   Final Test Loss                 : {final_loss:.4f}")

# â”€â”€ STEP 12: Classification Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ“‹ Generating Classification Report...")
all_preds, all_labels = [], []
model.eval()
with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        outputs = model(images)
        _, preds = outputs.max(1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.numpy())

print("\n" + classification_report(all_labels, all_preds, target_names=class_names))

# â”€â”€ STEP 13: Confusion Matrix â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix', fontsize=14, fontweight='bold')
plt.ylabel('True Label'); plt.xlabel('Predicted Label')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# â”€â”€ STEP 14: Visualize Predictions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def show_predictions(model, dataset, class_names, n=20, device='cpu'):
    model.eval()
    indices = np.random.choice(len(dataset), n, replace=False)
    fig, axes = plt.subplots(4, 5, figsize=(14, 11))
    fig.suptitle("Model Predictions (Green=Correct, Red=Wrong)", fontsize=13, fontweight='bold')

    with torch.no_grad():
        for i, ax in enumerate(axes.flat):
            img, true_label = dataset[indices[i]]
            output = model(img.unsqueeze(0).to(device))
            pred_label = output.argmax(1).item()
            conf = torch.softmax(output, dim=1).max().item() * 100

            img_np = img.squeeze().numpy() * 0.5 + 0.5
            ax.imshow(img_np, cmap='gray')
            color = 'green' if pred_label == true_label else 'red'
            ax.set_title(
                f"Pred: {class_names[pred_label]}\nTrue: {class_names[true_label]}\n{conf:.1f}%",
                color=color, fontsize=7.5
            )
            ax.axis('off')
    plt.tight_layout()
    plt.show()

show_predictions(model, raw_test, class_names, n=20, device=device)

# â”€â”€ STEP 15: Per-Class Accuracy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ“Š Per-Class Accuracy:")
cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)
per_class_acc = cm_norm.diagonal() * 100
print(f"\n{'Class':<15} {'Accuracy':>10}")
print("â”€" * 27)
for name, acc in zip(class_names, per_class_acc):
    bar = 'â–ˆ' * int(acc / 5)
    print(f"{name:<15} {acc:>8.1f}%  {bar}")

# Bar chart
plt.figure(figsize=(10, 5))
colors = ['#2ecc71' if a >= 90 else '#e67e22' if a >= 80 else '#e74c3c' for a in per_class_acc]
bars = plt.bar(class_names, per_class_acc, color=colors)
plt.xticks(rotation=45, ha='right')
plt.ylabel('Accuracy (%)'); plt.ylim([70, 100])
plt.title('Per-Class Accuracy', fontsize=13, fontweight='bold')
plt.axhline(y=np.mean(per_class_acc), color='blue', linestyle='--',
            label=f'Mean: {np.mean(per_class_acc):.1f}%')
for bar, acc in zip(bars, per_class_acc):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,
             f'{acc:.1f}%', ha='center', va='bottom', fontsize=8)
plt.legend(); plt.tight_layout(); plt.show()

# â”€â”€ STEP 16: Save Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'best_val_acc': best_val_acc,
    'class_names': class_names,
    'architecture': 'FashionCNN'
}, 'fashion_mnist_cnn_best.pth')

print("\nâœ… Model saved as 'fashion_mnist_cnn_best.pth'")
print(f"\n{'='*50}")
print(f"  ğŸ‰ FINAL RESULTS SUMMARY")
print(f"{'='*50}")
print(f"  Best Validation Accuracy : {best_val_acc:.2f}%")
print(f"  Final Test Accuracy      : {final_acc:.2f}%")
print(f"  Final Test Loss          : {final_loss:.4f}")
print(f"  Device Used              : {device}")
print(f"  Total Parameters         : {trainable:,}")
print(f"{'='*50}")